{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3dabbc",
   "metadata": {},
   "source": [
    "# Real-time Hand Gesture Recognition using MediaPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822ebfe",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import OpenCV, MediaPipe, NumPy, and other necessary libraries for webcam access and hand detection. \n",
    "\n",
    "**Please see the compability between mediapipe and python. The current version of mediapipe only support until python version 3.12.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b675d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully!\n",
      "OpenCV version: 4.11.0\n",
      "MediaPipe version: 0.10.21\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from collections import deque\n",
    "from IPython.display import Image, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"MediaPipe version: {mp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e915ea4",
   "metadata": {},
   "source": [
    "## 2. MediaPipe Hand Detection Setup\n",
    "\n",
    "Initialize MediaPipe Hands solution with detection and tracking confidence parameters for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37d38cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MediaPipe Hands initialized with 21 landmarks per hand\n",
      "Hand landmarks: ['WRIST', 'THUMB_CMC', 'THUMB_MCP', 'THUMB_IP', 'THUMB_TIP', 'INDEX_MCP', 'INDEX_PIP', 'INDEX_DIP', 'INDEX_TIP', 'MIDDLE_MCP', 'MIDDLE_PIP', 'MIDDLE_DIP', 'MIDDLE_TIP', 'RING_MCP', 'RING_PIP', 'RING_DIP', 'RING_TIP', 'PINKY_MCP', 'PINKY_PIP', 'PINKY_DIP', 'PINKY_TIP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765303292.189245 3377403 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M2\n"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Create Hands detector with confidence parameters\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,  # For real-time video\n",
    "    max_num_hands=2,  # Detect up to 2 hands\n",
    "    min_detection_confidence=0.7,  # Minimum confidence for hand detection\n",
    "    min_tracking_confidence=0.5  # Minimum confidence for hand tracking\n",
    ")\n",
    "\n",
    "# Hand landmark names (21 landmarks per hand)\n",
    "HAND_LANDMARKS = [\n",
    "    \"WRIST\", \"THUMB_CMC\", \"THUMB_MCP\", \"THUMB_IP\", \"THUMB_TIP\",\n",
    "    \"INDEX_MCP\", \"INDEX_PIP\", \"INDEX_DIP\", \"INDEX_TIP\",\n",
    "    \"MIDDLE_MCP\", \"MIDDLE_PIP\", \"MIDDLE_DIP\", \"MIDDLE_TIP\",\n",
    "    \"RING_MCP\", \"RING_PIP\", \"RING_DIP\", \"RING_TIP\",\n",
    "    \"PINKY_MCP\", \"PINKY_PIP\", \"PINKY_DIP\", \"PINKY_TIP\"\n",
    "]\n",
    "\n",
    "print(f\"âœ“ MediaPipe Hands initialized with {len(HAND_LANDMARKS)} landmarks per hand\")\n",
    "print(f\"Hand landmarks: {HAND_LANDMARKS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b92d3e",
   "metadata": {},
   "source": [
    "## 3. Theory of Hand Gesture Recognition\n",
    "\n",
    "### How MediaPipe Works\n",
    "\n",
    "MediaPipe detects hands by finding 21 key points (landmarks) on each hand. These landmarks are located at joints and fingertips.\n",
    "\n",
    "**The 21 Hand Landmarks:**\n",
    "- 1 wrist point\n",
    "- 4 points on each finger (thumb, index, middle, ring, pinky)\n",
    "\n",
    "Each point has (x, y, z) coordinates showing its position.\n",
    "\n",
    "**How Gestures are Detected:**\n",
    "\n",
    "We check if fingers are open or closed by comparing distances between landmarks:\n",
    "- If the tip is far from the base = finger is **open**\n",
    "- If the tip is close to the base = finger is **closed**\n",
    "\n",
    "**Examples:**\n",
    "- **Thumbs Up ğŸ‘** = Thumb pointing up, other fingers closed\n",
    "- **Peace âœŒï¸** = Index + middle fingers up, others closed  \n",
    "- **Pointing â˜ï¸** = Only index finger up\n",
    "- **Fist âœŠ** = All fingers closed\n",
    "- **Open Hand âœ‹** = All fingers open\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ea5e9",
   "metadata": {},
   "source": [
    "## 4. Utility Functions for Gesture Recognition\n",
    "\n",
    "Helper functions to calculate distances, angles, and detect finger extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa05d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Gesture classification functions defined successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1765303292.200371 3386783 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1765303292.212295 3386783 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "def distance(p1, p2):\n",
    "    \"\"\"Calculate Euclidean distance between two points.\"\"\"\n",
    "    return math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "def is_finger_extended(mcp, pip, tip):\n",
    "    \"\"\"Check if a finger is extended by comparing distances.\"\"\"\n",
    "    # If the distance from MCP to TIP is greater than MCP to PIP, finger is extended\n",
    "    return distance(mcp, tip) > distance(mcp, pip)\n",
    "\n",
    "def is_hand_open(landmarks):\n",
    "    \"\"\"Check if hand is open (all fingers extended).\"\"\"\n",
    "    thumb_extended = is_finger_extended(landmarks[2], landmarks[1], landmarks[4])\n",
    "    index_extended = is_finger_extended(landmarks[5], landmarks[6], landmarks[8])\n",
    "    middle_extended = is_finger_extended(landmarks[9], landmarks[10], landmarks[12])\n",
    "    ring_extended = is_finger_extended(landmarks[13], landmarks[14], landmarks[16])\n",
    "    pinky_extended = is_finger_extended(landmarks[17], landmarks[18], landmarks[20])\n",
    "    \n",
    "    return sum([thumb_extended, index_extended, middle_extended, ring_extended, pinky_extended]) >= 4\n",
    "\n",
    "def is_fist(landmarks):\n",
    "    \"\"\"Check if hand is forming a fist (all fingers closed).\"\"\"\n",
    "    thumb_extended = is_finger_extended(landmarks[2], landmarks[1], landmarks[4])\n",
    "    index_extended = is_finger_extended(landmarks[5], landmarks[6], landmarks[8])\n",
    "    middle_extended = is_finger_extended(landmarks[9], landmarks[10], landmarks[12])\n",
    "    ring_extended = is_finger_extended(landmarks[13], landmarks[14], landmarks[16])\n",
    "    pinky_extended = is_finger_extended(landmarks[17], landmarks[18], landmarks[20])\n",
    "    \n",
    "    return sum([thumb_extended, index_extended, middle_extended, ring_extended, pinky_extended]) <= 1\n",
    "\n",
    "def is_thumbs_up(landmarks):\n",
    "    \"\"\"Check if gesture is thumbs up.\"\"\"\n",
    "    # Thumb should be extended and pointing up (high y-coordinate)\n",
    "    thumb_extended = is_finger_extended(landmarks[2], landmarks[1], landmarks[4])\n",
    "    # Other fingers closed\n",
    "    index_extended = is_finger_extended(landmarks[5], landmarks[6], landmarks[8])\n",
    "    middle_extended = is_finger_extended(landmarks[9], landmarks[10], landmarks[12])\n",
    "    \n",
    "    # Thumb tip should be above thumb MCP (lower y value)\n",
    "    thumb_up = landmarks[4][1] < landmarks[2][1]\n",
    "    \n",
    "    return thumb_extended and thumb_up and not index_extended and not middle_extended\n",
    "\n",
    "def is_peace_sign(landmarks):\n",
    "    \"\"\"Check if gesture is peace sign (victory).\"\"\"\n",
    "    # Index and middle fingers extended, others closed\n",
    "    index_extended = is_finger_extended(landmarks[5], landmarks[6], landmarks[8])\n",
    "    middle_extended = is_finger_extended(landmarks[9], landmarks[10], landmarks[12])\n",
    "    ring_extended = is_finger_extended(landmarks[13], landmarks[14], landmarks[16])\n",
    "    pinky_extended = is_finger_extended(landmarks[17], landmarks[18], landmarks[20])\n",
    "\n",
    "def is_pointing(landmarks):\n",
    "    \"\"\"Check if gesture is pointing (index finger only).\"\"\"\n",
    "    index_extended = is_finger_extended(landmarks[5], landmarks[6], landmarks[8])\n",
    "    middle_extended = is_finger_extended(landmarks[9], landmarks[10], landmarks[12])\n",
    "    ring_extended = is_finger_extended(landmarks[13], landmarks[14], landmarks[16])\n",
    "    pinky_extended = is_finger_extended(landmarks[17], landmarks[18], landmarks[20])\n",
    "    \n",
    "    # Only index finger extended\n",
    "    return index_extended and not middle_extended and not ring_extended and not pinky_extended\n",
    "\n",
    "def classify_gesture(landmarks):\n",
    "    \"\"\"Classify hand gesture from landmarks.\"\"\"\n",
    "    if is_thumbs_up(landmarks):\n",
    "        return \"THUMBS UP ğŸ‘\", 0.9\n",
    "    elif is_peace_sign(landmarks):\n",
    "        return \"PEACE âœŒï¸\", 0.85\n",
    "    elif is_pointing(landmarks):\n",
    "        return \"POINTING â˜ï¸\", 0.8\n",
    "    elif is_fist(landmarks):\n",
    "        return \"FIST âœŠ\", 0.95\n",
    "    elif is_hand_open(landmarks):\n",
    "        return \"OPEN HAND âœ‹\", 0.9\n",
    "    else:\n",
    "        return \"NEUTRAL\", 0.5\n",
    "\n",
    "print(\"âœ“ Gesture classification functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d78b80d",
   "metadata": {},
   "source": [
    "## 5. Image Processing Functions\n",
    "\n",
    "Helper function to process images and detect hand gestures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f92f2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hand Gesture Detection from Images\n",
      "============================================================\n",
      "Looking for images in: hand_images\n",
      "\n",
      "Found 4 image(s) to process\n",
      "------------------------------------------------------------\n",
      "[1] âœ“ peace_sign_2.jpg\n",
      "     â†’ OPEN HAND âœ‹ (90% confidence)\n",
      "[2] âœ“ thumbs_up.jpg\n",
      "     â†’ THUMBS UP ğŸ‘ (90% confidence)\n",
      "     â†’ OPEN HAND âœ‹ (90% confidence)\n",
      "[3] âœ“ open_hand.jpg\n",
      "     â†’ OPEN HAND âœ‹ (90% confidence)\n",
      "[4] âœ“ peace_sign.jpg\n",
      "     â†’ OPEN HAND âœ‹ (90% confidence)\n",
      "\n",
      "============================================================\n",
      "âœ“ Processing complete! 4 image(s) processed\n",
      "âœ“ Results saved to: gesture_results/\n",
      "[4] âœ“ peace_sign.jpg\n",
      "     â†’ OPEN HAND âœ‹ (90% confidence)\n",
      "\n",
      "============================================================\n",
      "âœ“ Processing complete! 4 image(s) processed\n",
      "âœ“ Results saved to: gesture_results/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory for results\n",
    "output_dir = \"gesture_results\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Image directory containing hand gesture images\n",
    "image_dir = \"hand_images\"  # Change this to your image folder path\n",
    "\n",
    "# Statistics tracking\n",
    "detection_stats = {\n",
    "    'THUMBS UP ğŸ‘': 0,\n",
    "    'PEACE âœŒï¸': 0,\n",
    "    'POINTING â˜ï¸': 0,\n",
    "    'FIST âœŠ': 0,\n",
    "    'OPEN HAND âœ‹': 0,\n",
    "    'NEUTRAL': 0\n",
    "}\n",
    "\n",
    "detection_results = []\n",
    "\n",
    "# Supported image formats\n",
    "supported_formats = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "\n",
    "print(\"Hand Gesture Detection from Images\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Looking for images in: {image_dir}\")\n",
    "print()\n",
    "\n",
    "# Check if image directory exists\n",
    "if not os.path.exists(image_dir):\n",
    "    print(f\"âš ï¸  Image directory '{image_dir}' not found!\")\n",
    "    print(f\"Please create the folder and add hand gesture images.\")\n",
    "    print(f\"\\nExample usage:\")\n",
    "    print(f\"1. Create folder: mkdir hand_images\")\n",
    "    print(f\"2. Add images: place .jpg or .png files in hand_images/\")\n",
    "    print(f\"3. Re-run this cell\")\n",
    "else:\n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(image_dir) \n",
    "                   if os.path.isfile(os.path.join(image_dir, f)) \n",
    "                   and Path(f).suffix.lower() in supported_formats]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No image files found in '{image_dir}'\")\n",
    "        print(f\"Supported formats: {', '.join(supported_formats)}\")\n",
    "    else:\n",
    "        print(f\"Found {len(image_files)} image(s) to process\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Process each image\n",
    "        for idx, img_file in enumerate(image_files, 1):\n",
    "            img_path = os.path.join(image_dir, img_file)\n",
    "            \n",
    "            try:\n",
    "                # Read image\n",
    "                image = cv2.imread(img_path)\n",
    "                \n",
    "                if image is None:\n",
    "                    print(f\"[{idx}] âœ— Error reading: {img_file}\")\n",
    "                    continue\n",
    "                \n",
    "                # Convert BGR to RGB\n",
    "                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process with MediaPipe\n",
    "                results = hands.process(rgb_image)\n",
    "                \n",
    "                # Copy image for annotation\n",
    "                annotated_image = image.copy()\n",
    "                h, w, c = annotated_image.shape\n",
    "                \n",
    "                # Extract gesture info\n",
    "                gesture_info = {\n",
    "                    'filename': img_file,\n",
    "                    'hands_detected': 0,\n",
    "                    'gestures': []\n",
    "                }\n",
    "                \n",
    "                if results.multi_hand_landmarks and results.multi_handedness:\n",
    "                    for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                        # Extract landmarks\n",
    "                        landmarks = []\n",
    "                        for lm in hand_landmarks.landmark:\n",
    "                            landmarks.append((lm.x, lm.y, lm.z))\n",
    "                        \n",
    "                        # Classify gesture\n",
    "                        gesture, confidence = classify_gesture(landmarks)\n",
    "                        \n",
    "                        # Store info\n",
    "                        gesture_info['hands_detected'] += 1\n",
    "                        gesture_info['gestures'].append({\n",
    "                            'gesture': gesture,\n",
    "                            'confidence': confidence,\n",
    "                            'handedness': handedness.classification[0].label\n",
    "                        })\n",
    "                        \n",
    "                        # Update stats\n",
    "                        detection_stats[gesture] += 1\n",
    "                        \n",
    "                        # Draw landmarks\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            annotated_image,\n",
    "                            hand_landmarks,\n",
    "                            mp_hands.HAND_CONNECTIONS,\n",
    "                            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                            mp_drawing_styles.get_default_hand_connections_style()\n",
    "                        )\n",
    "                        \n",
    "                        # Add text annotations\n",
    "                        y_pos = 50 + gesture_info['hands_detected'] * 80\n",
    "                        cv2.putText(annotated_image, \n",
    "                                  f\"Hand {gesture_info['hands_detected']} ({handedness.classification[0].label})\", \n",
    "                                  (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                        cv2.putText(annotated_image, \n",
    "                                  f\"Gesture: {gesture}\", \n",
    "                                  (10, y_pos + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "                        cv2.putText(annotated_image, \n",
    "                                  f\"Confidence: {confidence:.2%}\", \n",
    "                                  (10, y_pos + 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)\n",
    "                \n",
    "                detection_results.append(gesture_info)\n",
    "                \n",
    "                # Save annotated image\n",
    "                output_path = os.path.join(output_dir, f\"detected_{img_file}\")\n",
    "                cv2.imwrite(output_path, annotated_image)\n",
    "                \n",
    "                # Print result\n",
    "                print(f\"[{idx}] âœ“ {img_file}\")\n",
    "                if gesture_info['hands_detected'] > 0:\n",
    "                    for g_info in gesture_info['gestures']:\n",
    "                        print(f\"     â†’ {g_info['gesture']} ({g_info['confidence']:.0%} confidence)\")\n",
    "                else:\n",
    "                    print(f\"     â†’ No hands detected\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[{idx}] âœ— Error processing {img_file}: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"âœ“ Processing complete! {len(image_files)} image(s) processed\")\n",
    "        print(f\"âœ“ Results saved to: {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf326ea",
   "metadata": {},
   "source": [
    "## 6. Test Output and Results Analysis\n",
    "\n",
    "Display detection statistics, visualize results, and analyze performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25fbd75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETECTION RESULTS\n",
      "============================================================\n",
      "Images processed: 4\n",
      "Total hands detected: 5\n",
      "\n",
      "ğŸ“„ peace_sign_2.jpg\n",
      "   â†’ OPEN HAND âœ‹ (90%)\n",
      "ğŸ“„ thumbs_up.jpg\n",
      "   â†’ THUMBS UP ğŸ‘ (90%)\n",
      "   â†’ OPEN HAND âœ‹ (90%)\n",
      "ğŸ“„ open_hand.jpg\n",
      "   â†’ OPEN HAND âœ‹ (90%)\n",
      "ğŸ“„ peace_sign.jpg\n",
      "   â†’ OPEN HAND âœ‹ (90%)\n"
     ]
    }
   ],
   "source": [
    "# Print detection summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETECTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if detection_results:\n",
    "    print(f\"Images processed: {len(detection_results)}\")\n",
    "    total_hands = sum(r['hands_detected'] for r in detection_results)\n",
    "    print(f\"Total hands detected: {total_hands}\")\n",
    "    print()\n",
    "    \n",
    "    for result in detection_results:\n",
    "        print(f\"ğŸ“„ {result['filename']}\")\n",
    "        if result['gestures']:\n",
    "            for g in result['gestures']:\n",
    "                print(f\"   â†’ {g['gesture']} ({g['confidence']:.0%})\")\n",
    "        else:\n",
    "            print(f\"   â†’ No hands detected\")\n",
    "else:\n",
    "    print(\"No results yet. Process images first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131de77",
   "metadata": {},
   "source": [
    "## 7. Display Detection Results\n",
    "\n",
    "View the annotated images with detected hand gestures and landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd82b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Detection results saved! (4 images)\n",
      "============================================================\n",
      "  â†’ detected_open_hand.jpg\n",
      "  â†’ detected_peace_sign.jpg\n",
      "  â†’ detected_peace_sign_2.jpg\n",
      "  â†’ detected_thumbs_up.jpg\n",
      "============================================================\n",
      "Annotated images are in: gesture_results/\n"
     ]
    }
   ],
   "source": [
    "# Display detection results status\n",
    "if os.path.exists(output_dir):\n",
    "    result_files = [f for f in os.listdir(output_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    if result_files:\n",
    "        print(f\"\\nâœ“ Detection results saved! ({len(result_files)} images)\")\n",
    "        print(\"=\" * 60)\n",
    "        for result_file in sorted(result_files):\n",
    "            print(f\"  â†’ {result_file}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Annotated images are in: {output_dir}/\")\n",
    "    else:\n",
    "        print(f\"No processed images found in '{output_dir}/'\")\n",
    "else:\n",
    "    print(f\"Results directory '{output_dir}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b7519",
   "metadata": {},
   "source": [
    "# Note about peace sign detection\n",
    "\n",
    "Peace sign images detected as OPEN HAND. This happens because the ring and pinky fingers are also extended in those images, so the detector classifies it as OPEN HAND (5 fingers extended) instead of PEACE sign. For accurate peace sign detection, the ring and pinky fingers need to be more clearly closed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
